{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkhaledaawad/Roomify-AI/blob/main/Roomify_GAN_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvrNMuBz3udI"
      },
      "source": [
        "# **Environment setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jKA6DepQjYe",
        "outputId": "934dfa16-62a4-4418-d2d4-5c66ebaa100c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXjr637rXvJA",
        "outputId": "30801403-7e52-460e-ac5d-a896dc290971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "1CG9e__V4Zh0",
        "outputId": "d31bc051-0f97-457e-aded-40b6a3d2a715"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6e2ee19f-c4fe-46ed-a48f-84bbefab0947\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6e2ee19f-c4fe-46ed-a48f-84bbefab0947\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "ref                                                                  title                                                     size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "-------------------------------------------------------------------  --------------------------------------------------  ----------  --------------------------  -------------  ---------  ---------------  \n",
            "atharvasoundankar/chocolate-sales                                    Chocolate Sales Data 📊🍫                                  14473  2025-03-19 03:51:40.270000          11523        203  1.0              \n",
            "adilshamim8/student-depression-dataset                               Student Depression Dataset                              467020  2025-03-13 03:12:30.423000           3881         63  1.0              \n",
            "abdulmalik1518/mobiles-dataset-2025                                  Mobiles Dataset (2025)                                   20314  2025-02-18 06:50:24.370000          17283        292  1.0              \n",
            "ak0212/anxiety-and-depression-mental-health-factors                  Anxiety and Depression Mental Health Factors             21687  2025-03-14 13:03:23.933000           1312         30  1.0              \n",
            "atharvasoundankar/mental-health-and-lifestyle-habits-2019-2024       🧠 Mental Health and Lifestyle Habits (2019-2024)         41947  2025-03-17 05:05:52.133000            913         24  1.0              \n",
            "mahmoudelhemaly/students-grading-dataset                             Student Performance & Behavior Dataset                  520428  2025-02-17 17:38:46.653000          12205        190  1.0              \n",
            "ankushpanday2/heart-attack-prediction-in-indonesia                   Heart Attack Prediction in Indonesia                   5398776  2025-03-11 15:19:28.123000           1025         30  1.0              \n",
            "bhargavchirumamilla/netflix-movies-and-tv-shows-till-2025            Netflix Movies and TV shows till 2025                  6471169  2025-03-04 01:06:12.240000           2130         33  1.0              \n",
            "zahidmughal2343/employee-data                                        Employee Data                                           379143  2025-03-08 19:36:42.953000           1651         30  1.0              \n",
            "atharvasoundankar/global-cybersecurity-threats-2015-2024             🌐 Global Cybersecurity Threats (2015-2024)               48178  2025-03-16 04:23:13.343000           1288         31  1.0              \n",
            "abdulmoiz12/amazon-stock-data-2025                                   Amazon Stock Data 2025                                  160519  2025-03-01 09:08:07.890000           1711         38  1.0              \n",
            "shohinurpervezshohan/freelancer-earnings-and-job-trends              Freelancer Earnings & Job Trends                         52906  2025-03-08 07:21:46.633000           1505         32  0.9411765        \n",
            "adilshamim8/personalized-learning-and-adaptive-education-dataset     Personalized Learning & Adaptive Education Dataset      168962  2025-02-25 09:49:37.340000           1075         24  1.0              \n",
            "ricgomes/global-fashion-retail-stores-dataset                        Global Fashion Retail Sales                          234910599  2025-03-19 18:37:15.857000           2033         30  1.0              \n",
            "shantanugarg274/heart-prediction-dataset-quantum                     Heart Prediction Dataset (Quantum)                        6993  2025-02-28 14:00:06.887000           1444         29  1.0              \n",
            "aradhanahirapara/income-survey-finance-analysis                      Income Survey | Finance Analysis                       1826775  2025-03-20 19:53:01.633000           1137         25  0.7647059        \n",
            "atharvasoundankar/viral-social-media-trends-and-engagement-analysis  🚀 Viral Social Media Trends & Engagement Analysis       107245  2025-03-10 04:51:48.123000           2624         47  1.0              \n",
            "atharvasoundankar/global-water-consumption-dataset-2000-2024         Global Water Consumption Dataset (2000-2024) 🌍💧          17077  2025-03-17 04:50:38.880000           2261         37  1.0              \n",
            "willianoliveiragibin/grocery-inventory                               Grocery Inventory                                        50801  2025-03-16 21:03:55.307000           1110         32  1.0              \n",
            "adilshamim8/temperature                                              Global Environmental Trends 2000-2024                    22391  2025-03-14 08:11:56.090000           1074         23  1.0              \n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euxXDzw-30dJ"
      },
      "source": [
        "# **COCO2017 dataset preperation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWAknhof24Qf",
        "outputId": "2cd8f13d-4980-42a4-80ec-7c781c5cafce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved 591753 caption entries to /content/drive/MyDrive/Roomify/data/coco/prompts.csv\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Path to your annotations file\n",
        "coco_caption_path = \"/content/drive/MyDrive/Roomify/data/coco/annotations/captions_train2017.json\"\n",
        "\n",
        "# Load JSON\n",
        "with open(coco_caption_path, 'r') as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "# Map image_id to file_name\n",
        "image_id_to_file = {img['id']: img['file_name'] for img in coco_data['images']}\n",
        "\n",
        "# Extract caption entries\n",
        "rows = []\n",
        "for ann in coco_data['annotations']:\n",
        "    image_id = ann['image_id']\n",
        "    caption = ann['caption']\n",
        "    image_name = image_id_to_file[image_id]\n",
        "    rows.append([image_name, caption, \"\"])  # No mask path for COCO\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(rows, columns=[\"image_name\", \"prompt_text\", \"mask_path\"])\n",
        "csv_path = \"/content/drive/MyDrive/Roomify/data/coco/prompts.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"✅ Saved {len(df)} caption entries to {csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiPwARfCLJ3A",
        "outputId": "1d80eed9-023a-4100-e90c-cfd0dea430b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         image_name                                        prompt_text  \\\n",
            "0  000000203564.jpg  A bicycle replica with a clock as the front wh...   \n",
            "1  000000322141.jpg  A room with blue walls and a white sink and door.   \n",
            "2  000000016977.jpg  A car that seems to be parked illegally behind...   \n",
            "\n",
            "   mask_path  \n",
            "0        NaN  \n",
            "1        NaN  \n",
            "2        NaN  \n"
          ]
        }
      ],
      "source": [
        "df_coco = pd.read_csv(\"/content/drive/MyDrive/Roomify/data/coco/prompts.csv\")\n",
        "print(df_coco.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LdnzAeCMdi8",
        "outputId": "878797ba-17a6-4698-bdac-385170ecf25f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "000000000009.jpg\n",
            "000000000025.jpg\n",
            "000000000030.jpg\n",
            "000000000034.jpg\n",
            "000000000049.jpg\n"
          ]
        }
      ],
      "source": [
        "!ls \"/content/drive/MyDrive/Roomify/data/coco/images/train2017\" | head -n 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOQvoUd7Lax6",
        "outputId": "824d514a-ffb4-44ca-a59d-8a6be908417e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Fixed COCO prompts.csv with 76783 entries saved to: /content/drive/MyDrive/Roomify/data/coco/prompts.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# COCO image folder\n",
        "coco_dir = \"/content/drive/MyDrive/Roomify/data/coco/images/train2017\"\n",
        "output_csv = \"/content/drive/MyDrive/Roomify/data/coco/prompts.csv\"\n",
        "\n",
        "rows = []\n",
        "for file in os.listdir(coco_dir):\n",
        "    if file.endswith(\".jpg\"):\n",
        "        image_name = f\"coco/images/train2017/{file}\"\n",
        "        prompt = \"A realistic indoor scene\"  # (optional: you can update this later)\n",
        "        rows.append([image_name, prompt, \"\"])\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\"image_name\", \"prompt_text\", \"mask_path\"])\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"✅ Fixed COCO prompts.csv with {len(df)} entries saved to: {output_csv}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4fq3Y8L3_RE"
      },
      "source": [
        "# **ADE20K dataset downloading and preperation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYA-PQym4F2v",
        "outputId": "bbc4a704-590e-498e-b8ea-bdd7e8428c45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/awsaf49/ade20k-dataset\n",
            "License(s): unknown\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d awsaf49/ade20k-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ti_3Ey846xm"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/Roomify/data/ade20k/\n",
        "!mv /content/ade20k-dataset.zip /content/drive/MyDrive/Roomify/data/ade20k/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTJTsuDE5Qbw"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/drive/MyDrive/Roomify/data/ade20k/ade20k-dataset.zip -d /content/drive/MyDrive/Roomify/data/ade20k/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4lHIDpS7dne",
        "outputId": "38d2f6b9-58e2-4db3-b9f2-ba6f08f5fcf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ADE_train_00000001.jpg\n",
            "ADE_train_00000002.jpg\n",
            "ADE_train_00000003.jpg\n",
            "ADE_train_00000004.jpg\n",
            "ADE_train_00000005.jpg\n",
            "ADE_train_00000006.jpg\n",
            "ADE_train_00000007.jpg\n",
            "ADE_train_00000008.jpg\n",
            "ADE_train_00000009.jpg\n",
            "ADE_train_00000010.jpg\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Roomify/data/ade20k/ADEChallengeData2016/images/training | head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUwGzStO5gPD",
        "outputId": "c260c3a0-e78c-4bc8-88f4-74f65a0d46e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved 20210 entries to prompts.csv for ADE20K\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Paths\n",
        "images_dir = \"/content/drive/MyDrive/Roomify/data/ade20k/ADEChallengeData2016/images/training\"\n",
        "masks_dir = \"/content/drive/MyDrive/Roomify/data/ade20k/ADEChallengeData2016/annotations/training\"\n",
        "output_csv = \"/content/drive/MyDrive/Roomify/data/ade20k/prompts.csv\"\n",
        "\n",
        "# Build rows\n",
        "data = []\n",
        "for file in os.listdir(images_dir):\n",
        "    if file.endswith(\".jpg\"):\n",
        "        image_name = file\n",
        "        mask_name = file.replace(\".jpg\", \".png\")\n",
        "        mask_path = f\"ADEChallengeData2016/annotations/training/{mask_name}\"\n",
        "        prompt_text = \"A scene with detailed semantic layout.\"  # Placeholder\n",
        "        data.append([image_name, prompt_text, mask_path])\n",
        "\n",
        "# Save CSV\n",
        "df = pd.DataFrame(data, columns=[\"image_name\", \"prompt_text\", \"mask_path\"])\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"✅ Saved {len(df)} entries to prompts.csv for ADE20K\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO12FVpULj5K",
        "outputId": "0366fb30-0910-4b50-8910-8b2797b463bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         image_name                                        prompt_text  \\\n",
            "0  000000203564.jpg  A bicycle replica with a clock as the front wh...   \n",
            "1  000000322141.jpg  A room with blue walls and a white sink and door.   \n",
            "2  000000016977.jpg  A car that seems to be parked illegally behind...   \n",
            "\n",
            "   mask_path  \n",
            "0        NaN  \n",
            "1        NaN  \n",
            "2        NaN  \n"
          ]
        }
      ],
      "source": [
        "df_ade20k = pd.read_csv(\"/content/drive/MyDrive/Roomify/data/ade20k/prompts.csv\")\n",
        "print(df_coco.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4DdjY_HNOGF",
        "outputId": "c4722a8e-eef4-4827-e10a-c3ffbb845700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Fixed ADE20K prompts.csv with 20210 entries saved to: /content/drive/MyDrive/Roomify/data/ade20k/prompts.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "img_dir = \"/content/drive/MyDrive/Roomify/data/ade20k/ADEChallengeData2016/images/training\"\n",
        "mask_dir = \"/content/drive/MyDrive/Roomify/data/ade20k/ADEChallengeData2016/annotations/training\"\n",
        "output_csv = \"/content/drive/MyDrive/Roomify/data/ade20k/prompts.csv\"\n",
        "\n",
        "rows = []\n",
        "for file in os.listdir(img_dir):\n",
        "    if file.endswith(\".jpg\"):\n",
        "        image_path = f\"ade20k/ADEChallengeData2016/images/training/{file}\"\n",
        "        mask_path = f\"ade20k/ADEChallengeData2016/annotations/training/{file.replace('.jpg', '_seg.png')}\"\n",
        "        prompt = \"A scene with detailed semantic layout\"\n",
        "        rows.append([image_path, prompt, mask_path])\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\"image_name\", \"prompt_text\", \"mask_path\"])\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"✅ Fixed ADE20K prompts.csv with {len(df)} entries saved to: {output_csv}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZuSr_wz7lrq"
      },
      "source": [
        "# **Furniture Dataset preperation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK7PL_-47ovZ",
        "outputId": "71cce55e-abf4-4db6-e88e-fd6b59bd32eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/udaysankarmukherjee/furniture-image-dataset\n",
            "License(s): apache-2.0\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d udaysankarmukherjee/furniture-image-dataset -p /content/drive/MyDrive/Roomify/data/furniture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "503jxCMo7uhH"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/drive/MyDrive/Roomify/data/furniture/furniture-image-dataset.zip -d /content/drive/MyDrive/Roomify/data/furniture/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLo7-eAx8_Hz",
        "outputId": "d42035fb-dfbd-47e7-a5f4-8236e4555a92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "almirah_dataset\n",
            "chair_dataset\n",
            "fridge dataset\n",
            "furniture-image-dataset.zip\n",
            "table dataset\n",
            "tv dataset\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Roomify/data/furniture/ | head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdBhG2Q29LWt",
        "outputId": "24726b28-bed4-46f8-9ea5-9efde2c53a3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved 15000 entries to: /content/drive/MyDrive/Roomify/data/furniture/prompts.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "root_dir = \"/content/drive/MyDrive/Roomify/data/furniture\"\n",
        "output_csv = f\"{root_dir}/prompts.csv\"\n",
        "\n",
        "rows = []\n",
        "for category in os.listdir(root_dir):\n",
        "    category_path = os.path.join(root_dir, category)\n",
        "    if os.path.isdir(category_path) and not category.endswith(\".zip\"):\n",
        "        for file in os.listdir(category_path):\n",
        "            if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "                image_path = f\"{category}/{file}\"\n",
        "                clean_category = category.replace(\"_\", \" \").replace(\"dataset\", \"\").strip()\n",
        "                prompt = f\"A modern {clean_category}\"\n",
        "                rows.append([image_path, prompt, \"\"])\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\"image_name\", \"prompt_text\", \"mask_path\"])\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"✅ Saved {len(df)} entries to: {output_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "383rZXTLNuTC",
        "outputId": "eb053aa3-84dc-490e-efe5-b0a90859b2ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Fixed Furniture prompts.csv with 15000 entries saved to: /content/drive/MyDrive/Roomify/data/furniture/prompts.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "root_dir = \"/content/drive/MyDrive/Roomify/data/furniture\"\n",
        "output_csv = f\"{root_dir}/prompts.csv\"\n",
        "\n",
        "rows = []\n",
        "for category in os.listdir(root_dir):\n",
        "    category_path = os.path.join(root_dir, category)\n",
        "    if os.path.isdir(category_path) and not category.endswith(\".zip\"):\n",
        "        for file in os.listdir(category_path):\n",
        "            if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "                image_path = f\"furniture/{category}/{file}\"\n",
        "                prompt = f\"A modern {category.replace('_', ' ').replace('dataset', '').strip()}\"\n",
        "                rows.append([image_path, prompt, \"\"])\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\"image_name\", \"prompt_text\", \"mask_path\"])\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"✅ Fixed Furniture prompts.csv with {len(df)} entries saved to: {output_csv}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdFnMABK9uuA"
      },
      "source": [
        "# **Interior Design dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcPGAw7j9y-x",
        "outputId": "df350430-2816-404a-99bc-5d22f6316ee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/aishahsofea/interior-design\n",
            "License(s): copyright-authors\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d aishahsofea/interior-design -p /content/drive/MyDrive/Roomify/data/interior_design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DNQzjNd9_DG"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/drive/MyDrive/Roomify/data/interior_design/interior-design.zip -d /content/drive/MyDrive/Roomify/data/interior_design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSwcC99A-GGo",
        "outputId": "b00b1edb-b6f1-403f-f158-621c1551deab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "interior-design.zip\n",
            "resized_images\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Roomify/data/interior_design | head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mH16xEz-Uke",
        "outputId": "e707eb4a-2ad0-4535-ecb9-12d25b704ee3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Fixed Interior Design prompts.csv with 4147 entries saved to: /content/drive/MyDrive/Roomify/data/interior_design/prompts.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "img_dir = \"/content/drive/MyDrive/Roomify/data/interior_design/resized_images\"\n",
        "output_csv = \"/content/drive/MyDrive/Roomify/data/interior_design/prompts.csv\"\n",
        "\n",
        "rows = []\n",
        "for file in os.listdir(img_dir):\n",
        "    if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "        image_name = f\"interior_design/resized_images/{file}\"\n",
        "        prompt = \"A beautifully designed interior room\"\n",
        "        rows.append([image_name, prompt, \"\"])\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\"image_name\", \"prompt_text\", \"mask_path\"])\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"✅ Fixed Interior Design prompts.csv with {len(df)} entries saved to: {output_csv}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5H0Qs2B-6zz"
      },
      "source": [
        "# **SUN_RGBD Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1zsBDwr-_Ne",
        "outputId": "02e843f9-3ee4-495b-9e4c-806e284f7689"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/thanhbnhphan/sun-rgbd-2d\n",
            "License(s): MIT\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d thanhbnhphan/sun-rgbd-2d -p /content/drive/MyDrive/Roomify/data/sun_rgbd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95yrKyuW_Ylp"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/drive/MyDrive/Roomify/data/sun_rgbd/sun-rgbd-2d.zip -d /content/drive/MyDrive/Roomify/data/sun_rgbd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvdd_oLDBAmv",
        "outputId": "c7821256-48d0-4ef0-c46a-164e5d162215"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "depth\n",
            "depth_bfx\n",
            "image\n",
            "info.json\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Roomify/data/sun_rgbd/MYSUN | head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXuNVoooDPxl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "img_dir = \"/content/drive/MyDrive/Roomify/data/sun_rgbd/MYSUN/image\"\n",
        "output_csv = \"/content/drive/MyDrive/Roomify/data/sun_rgbd/prompts.csv\"\n",
        "\n",
        "rows = []\n",
        "for file in os.listdir(img_dir):\n",
        "    if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "        image_name = f\"sun_rgbd/MYSUN/image/{file}\"\n",
        "        prompt = \"An indoor scene with depth and layout\"\n",
        "        rows.append([image_name, prompt, \"\"])\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\"image_name\", \"prompt_text\", \"mask_path\"])\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"✅ Fixed SUN RGB-D prompts.csv with {len(df)} entries saved to: {output_csv}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tas8m9fDhr4"
      },
      "source": [
        "# **Merge all datasets prompts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8kxGk8ZDnoL",
        "outputId": "52c449d6-60d7-43b2-8a69-182bbac421f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Unified prompts.csv created with 641445 entries at: /content/drive/MyDrive/Roomify/data/unified_prompts.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "paths = {\n",
        "    \"coco\": \"/content/drive/MyDrive/Roomify/data/coco/prompts.csv\",\n",
        "    \"ade20k\": \"/content/drive/MyDrive/Roomify/data/ade20k/prompts.csv\",\n",
        "    \"furniture\": \"/content/drive/MyDrive/Roomify/data/furniture/prompts.csv\",\n",
        "    \"interior\": \"/content/drive/MyDrive/Roomify/data/interior_design/prompts.csv\",\n",
        "    \"sunrgbd\": \"/content/drive/MyDrive/Roomify/data/sun_rgbd/prompts.csv\",\n",
        "}\n",
        "\n",
        "df_all = []\n",
        "\n",
        "for name, path in paths.items():\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.dropna(subset=[\"image_name\", \"prompt_text\"])\n",
        "    df[\"source\"] = name\n",
        "    df_all.append(df)\n",
        "\n",
        "df_merged = pd.concat(df_all, ignore_index=True)\n",
        "\n",
        "# Final path\n",
        "final_csv = \"/content/drive/MyDrive/Roomify/data/unified_prompts.csv\"\n",
        "df_merged.to_csv(final_csv, index=False)\n",
        "\n",
        "print(f\"✅ Unified prompts.csv created with {len(df_merged)} entries at: {final_csv}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYrJ0w48IpjX"
      },
      "source": [
        "# **Preprocessing data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sceyAPIlFw7t",
        "outputId": "3806f6e0-bd18-4bc7-f2fb-65a0e50ecde6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟢 Resuming from index: 97171\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing (resumed): 100%|██████████| 24474/24474 [2:16:05<00:00,  3.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Resumed preprocessing complete! CSV now has 24474 rows.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==== CONFIG ====\n",
        "input_csv = \"/content/drive/MyDrive/Roomify/data/unified_prompts.csv\"\n",
        "input_base = \"/content/drive/MyDrive/Roomify/data\"\n",
        "output_base = \"/content/drive/MyDrive/Roomify/data/processed\"\n",
        "output_images = os.path.join(output_base, \"images\")\n",
        "output_masks = os.path.join(output_base, \"masks\")\n",
        "output_csv = os.path.join(output_base, \"unified_prompts.csv\")\n",
        "target_size = (256, 256)\n",
        "\n",
        "# ==== SETUP ====\n",
        "os.makedirs(output_images, exist_ok=True)\n",
        "os.makedirs(output_masks, exist_ok=True)\n",
        "\n",
        "# ==== LOAD DATA ====\n",
        "df = pd.read_csv(input_csv)\n",
        "processed_rows = []\n",
        "\n",
        "# ==== LOOP ====\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
        "    image_rel = row[\"image_name\"]\n",
        "    prompt = row[\"prompt_text\"]\n",
        "    mask_rel = row[\"mask_path\"]\n",
        "\n",
        "    # === IMAGE ===\n",
        "    image_path = os.path.join(input_base, image_rel)\n",
        "    try:\n",
        "        img = Image.open(image_path).convert(\"RGB\").resize(target_size)\n",
        "        img_name = f\"img_{idx:06d}.jpg\"\n",
        "        img.save(os.path.join(output_images, img_name), \"JPEG\")\n",
        "    except Exception as e:\n",
        "        print(f\"[Image Error] Skipped row {idx}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # === MASK ===\n",
        "    mask_name = \"\"\n",
        "    if isinstance(mask_rel, str) and mask_rel.strip():\n",
        "        mask_path = os.path.join(input_base, mask_rel)\n",
        "        try:\n",
        "            mask = Image.open(mask_path).convert(\"L\").resize(target_size)\n",
        "            mask_name = f\"mask_{idx:06d}.png\"\n",
        "            mask.save(os.path.join(output_masks, mask_name), \"PNG\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Mask Error] Row {idx} mask skipped: {e}\")\n",
        "            mask_name = \"\"\n",
        "\n",
        "    # === RECORD ROW ===\n",
        "    processed_rows.append([\n",
        "        f\"images/{img_name}\",\n",
        "        prompt,\n",
        "        f\"masks/{mask_name}\" if mask_name else \"\"\n",
        "    ])\n",
        "\n",
        "# ==== SAVE CSV ====\n",
        "df_out = pd.DataFrame(processed_rows, columns=[\"image_name\", \"prompt_text\", \"mask_path\"])\n",
        "df_out.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"\\n✅ Preprocessing complete.\")\n",
        "print(f\"📁 Final CSV: {output_csv}\")\n",
        "print(f\"📸 Processed images: {len(df_out)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQSW9oVGGf4I",
        "outputId": "769853e3-3002-4f5c-e1ba-0b1ac5ead7ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         image_name                                        prompt_text  \\\n",
            "0  000000203564.jpg  A bicycle replica with a clock as the front wh...   \n",
            "1  000000322141.jpg  A room with blue walls and a white sink and door.   \n",
            "2  000000016977.jpg  A car that seems to be parked illegally behind...   \n",
            "\n",
            "  mask_path source  \n",
            "0       NaN   coco  \n",
            "1       NaN   coco  \n",
            "2       NaN   coco  \n",
            "000000203564.jpg\n",
            "False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-34-7f757517f96e>:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"/content/drive/MyDrive/Roomify/data/unified_prompts.csv\")\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Roomify/data/unified_prompts.csv\")\n",
        "print(df.head(3))\n",
        "print(df.iloc[0]['image_name'])\n",
        "print(os.path.exists(\"/content/drive/MyDrive/Roomify/data/\" + df.iloc[0]['image_name']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zx1zebrHLA6",
        "outputId": "08ca458f-fb9a-4ed4-dd9a-58ea1780b7fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Rebuilt unified_prompts.csv with 641445 rows.\n",
            "📄 Saved to: /content/drive/MyDrive/Roomify/data/unified_prompts.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Original prompts.csv files with correct image paths\n",
        "paths = {\n",
        "    \"coco\": \"/content/drive/MyDrive/Roomify/data/coco/prompts.csv\",\n",
        "    \"ade20k\": \"/content/drive/MyDrive/Roomify/data/ade20k/prompts.csv\",\n",
        "    \"furniture\": \"/content/drive/MyDrive/Roomify/data/furniture/prompts.csv\",\n",
        "    \"interior\": \"/content/drive/MyDrive/Roomify/data/interior_design/prompts.csv\",\n",
        "    \"sunrgbd\": \"/content/drive/MyDrive/Roomify/data/sun_rgbd/prompts.csv\",\n",
        "}\n",
        "\n",
        "# Load and tag each one with its source\n",
        "df_all = []\n",
        "for name, path in paths.items():\n",
        "    df = pd.read_csv(path)\n",
        "    df[\"source\"] = name\n",
        "    df_all.append(df)\n",
        "\n",
        "# Merge into one big CSV again\n",
        "df_merged = pd.concat(df_all, ignore_index=True)\n",
        "\n",
        "# Save the clean, fixed version\n",
        "output_path = \"/content/drive/MyDrive/Roomify/data/unified_prompts.csv\"\n",
        "df_merged.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"✅ Rebuilt unified_prompts.csv with {len(df_merged)} rows.\")\n",
        "print(f\"📄 Saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-VMZTmIIGTP",
        "outputId": "2efc4e1e-c61c-45e1-e4bb-85645829cc2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         image_name                                        prompt_text  \\\n",
            "0  000000203564.jpg  A bicycle replica with a clock as the front wh...   \n",
            "1  000000322141.jpg  A room with blue walls and a white sink and door.   \n",
            "2  000000016977.jpg  A car that seems to be parked illegally behind...   \n",
            "\n",
            "  mask_path source  \n",
            "0       NaN   coco  \n",
            "1       NaN   coco  \n",
            "2       NaN   coco  \n",
            "Checking path: /content/drive/MyDrive/Roomify/data/000000203564.jpg\n",
            "Exists? False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-35-ca570e74609f>:5: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"/content/drive/MyDrive/Roomify/data/unified_prompts.csv\")\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Load the rebuilt CSV\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Roomify/data/unified_prompts.csv\")\n",
        "\n",
        "# Show first few rows\n",
        "print(df.head(3))\n",
        "\n",
        "# Test if the first file exists\n",
        "test_path = \"/content/drive/MyDrive/Roomify/data/\" + df.iloc[0][\"image_name\"]\n",
        "print(\"Checking path:\", test_path)\n",
        "print(\"Exists?\", os.path.exists(test_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6F-sjZeIZXb",
        "outputId": "2423c053-acfe-417c-8f53-3dc285e182fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Fixed unified_prompts.csv written with 641445 valid rows.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "paths = {\n",
        "    \"coco\": \"/content/drive/MyDrive/Roomify/data/coco/prompts.csv\",\n",
        "    \"ade20k\": \"/content/drive/MyDrive/Roomify/data/ade20k/prompts.csv\",\n",
        "    \"furniture\": \"/content/drive/MyDrive/Roomify/data/furniture/prompts.csv\",\n",
        "    \"interior\": \"/content/drive/MyDrive/Roomify/data/interior_design/prompts.csv\",\n",
        "    \"sunrgbd\": \"/content/drive/MyDrive/Roomify/data/sun_rgbd/prompts.csv\",\n",
        "}\n",
        "\n",
        "df_all = []\n",
        "for name, path in paths.items():\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.dropna(subset=[\"image_name\", \"prompt_text\"])  # drop broken rows\n",
        "    df[\"source\"] = name\n",
        "    df_all.append(df)\n",
        "\n",
        "df_merged = pd.concat(df_all, ignore_index=True)\n",
        "\n",
        "# ✅ Make sure paths are relative to `/Roomify/data/`\n",
        "# DO NOT strip folders!\n",
        "df_merged.to_csv(\"/content/drive/MyDrive/Roomify/data/unified_prompts.csv\", index=False)\n",
        "\n",
        "print(f\"✅ Fixed unified_prompts.csv written with {len(df_merged)} valid rows.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w047esw6JZnp",
        "outputId": "b1c30c33-90fc-4975-f205-152ae8135d7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image: 000000203564.jpg\n",
            "Exists? False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-38-e87808fe691d>:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"/content/drive/MyDrive/Roomify/data/unified_prompts.csv\")\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Roomify/data/unified_prompts.csv\")\n",
        "\n",
        "first_image = df.iloc[0][\"image_name\"]\n",
        "full_path = \"/content/drive/MyDrive/Roomify/data/\" + first_image\n",
        "\n",
        "print(\"Image:\", first_image)\n",
        "print(\"Exists?\", os.path.exists(full_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJWf--T3I6XC"
      },
      "outputs": [],
      "source": [
        "# from PIL import Image\n",
        "# import os\n",
        "# from torchvision import transforms\n",
        "\n",
        "# source_dir = \"/content/drive/MyDrive/Roomify/data/processed/images\"\n",
        "# target_dir = \"/content/drive/MyDrive/Roomify/data/processed_512/images\"\n",
        "# os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((512, 512)),\n",
        "#     transforms.ToTensor()\n",
        "# ])\n",
        "\n",
        "# for file in os.listdir(source_dir):\n",
        "#     if file.endswith((\".jpg\", \".png\")):\n",
        "#         try:\n",
        "#             img_path = os.path.join(source_dir, file)\n",
        "#             image = Image.open(img_path).convert(\"RGB\")\n",
        "#             img_tensor = transform(image)\n",
        "#             img_output_path = os.path.join(target_dir, file)\n",
        "#             transforms.ToPILImage()(img_tensor).save(img_output_path)\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error processing {file}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ujaq9e9sJ4B_"
      },
      "source": [
        "# **Debugging**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk8PDz-KJ6n4",
        "outputId": "baef8e44-76fe-48a6-d7e6-85e6df80d1d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Valid image paths: 126475 / 126475\n",
            "🟥 Missing images: 0\n",
            "🟨 Missing masks: 20210\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Load CSV\n",
        "csv_path = \"/content/drive/MyDrive/Roomify/data/unified_prompts.csv\"\n",
        "base_dir = \"/content/drive/MyDrive/Roomify/data\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Validate image and mask paths\n",
        "missing_images = []\n",
        "missing_masks = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    img_path = os.path.join(base_dir, row[\"image_name\"])\n",
        "    if not os.path.exists(img_path):\n",
        "        missing_images.append(row[\"image_name\"])\n",
        "\n",
        "    mask = row[\"mask_path\"]\n",
        "    if isinstance(mask, str) and mask.strip():\n",
        "        mask_path = os.path.join(base_dir, mask)\n",
        "        if not os.path.exists(mask_path):\n",
        "            missing_masks.append(mask)\n",
        "\n",
        "print(f\"✅ Valid image paths: {len(df) - len(missing_images)} / {len(df)}\")\n",
        "print(f\"🟥 Missing images: {len(missing_images)}\")\n",
        "print(f\"🟨 Missing masks: {len(missing_masks)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "698oPBmxKVee",
        "outputId": "986bf38e-25b4-49ef-a116-7a711f04cf2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ Missing masks: 20210\n"
          ]
        }
      ],
      "source": [
        "missing_masks = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    mask_path = row[\"mask_path\"]\n",
        "    if isinstance(mask_path, str) and mask_path.strip():\n",
        "        full_path = os.path.join(base_dir, mask_path)\n",
        "        if not os.path.exists(full_path):\n",
        "            missing_masks.append(mask_path)\n",
        "\n",
        "print(f\"❌ Missing masks: {len(missing_masks)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqrXrSVjKcrH",
        "outputId": "7597a5c1-5f2a-4964-9306-07242ba152a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Filtered CSV with 0 valid rows saved.\n"
          ]
        }
      ],
      "source": [
        "def file_exists(row):\n",
        "    return os.path.exists(os.path.join(base_dir, row[\"image_name\"]))\n",
        "\n",
        "df_valid = df[df.apply(file_exists, axis=1)]\n",
        "df_valid.to_csv(csv_path, index=False)\n",
        "print(f\"✅ Filtered CSV with {len(df_valid)} valid rows saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7lqJbFUiez_"
      },
      "source": [
        "# **RoomifyDataset Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSVtgkE5irtP",
        "outputId": "16f3ab40-f28a-47de-b7d1-2d9778d1bf4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBthX8VciuN0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class RoomifyDataset(Dataset):\n",
        "    def __init__(self, csv_path, root_dir, transform=None, return_mask=True):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.return_mask = return_mask\n",
        "\n",
        "        self.image_dir = os.path.join(root_dir, \"images\")\n",
        "        self.mask_dir = os.path.join(root_dir, \"masks\")\n",
        "\n",
        "        self.default_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        image_path = os.path.join(self.root_dir, row[\"image_name\"])\n",
        "        prompt_text = row[\"prompt_text\"]\n",
        "        mask_path = row[\"mask_path\"]\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image = self.transform(image) if self.transform else self.default_transform(image)\n",
        "\n",
        "        # Load mask or return empty tensor\n",
        "        if self.return_mask and isinstance(mask_path, str) and len(mask_path.strip()) > 0:\n",
        "            full_mask_path = os.path.join(self.root_dir, mask_path)\n",
        "            try:\n",
        "                mask = Image.open(full_mask_path).convert(\"L\")\n",
        "                mask = transforms.ToTensor()(mask)  # 1xHxW\n",
        "            except:\n",
        "                mask = torch.zeros((1, 256, 256))  # fallback\n",
        "        else:\n",
        "            mask = torch.zeros((1, 256, 256))\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"text\": prompt_text,\n",
        "            \"mask\": mask,\n",
        "            \"index\": idx\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Io0DrUQNFeMD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def custom_collate(batch):\n",
        "    # Filter out any None samples\n",
        "    batch = [item for item in batch if item is not None]\n",
        "\n",
        "    # If batch is empty, raise an error\n",
        "    if len(batch) == 0:\n",
        "        raise ValueError(\"All items in the batch are None.\")\n",
        "\n",
        "    images = torch.stack([item[\"image\"] for item in batch])\n",
        "    texts = [item[\"text\"] for item in batch]\n",
        "\n",
        "    # Handle masks\n",
        "    if batch[0][\"mask\"] is not None:\n",
        "        masks = torch.stack([item[\"mask\"] for item in batch])\n",
        "    else:\n",
        "        masks = torch.zeros((len(batch), 1, 256, 256))  # fallback dummy masks\n",
        "\n",
        "    return {\n",
        "        \"image\": images,\n",
        "        \"text\": texts,\n",
        "        \"mask\": masks\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc3tkhiX346d"
      },
      "source": [
        "**data Loader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRSWP79F6byw",
        "outputId": "c2f5b1bc-d2b4-4c09-8db2-4075879c8e12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Batch loaded!\n",
            "Image batch shape: torch.Size([16, 3, 256, 256])\n",
            "Text batch: ['A beautifully designed interior room', 'A modern table']\n",
            "Mask batch shape: torch.Size([16, 1, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Initialize the dataset\n",
        "dataset = RoomifyDataset(\n",
        "    csv_path=\"/content/drive/MyDrive/Roomify/data/processed/unified_prompts.csv\",\n",
        "    root_dir=\"/content/drive/MyDrive/Roomify/data/processed/\"\n",
        ")\n",
        "\n",
        "# Create DataLoader\n",
        "roomify_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=custom_collate\n",
        ")\n",
        "\n",
        "# Preview a batch\n",
        "batch = next(iter(roomify_loader))\n",
        "print(\"✅ Batch loaded!\")\n",
        "print(\"Image batch shape:\", batch[\"image\"].shape)     # [B, 3, 256, 256]\n",
        "print(\"Text batch:\", batch[\"text\"][:2])               # Two sample prompts\n",
        "print(\"Mask batch shape:\", batch[\"mask\"].shape)       # [B, 1, 256, 256]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh0dAdWx9Wbq"
      },
      "source": [
        "**data module phase**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TullnTrqQ1TD",
        "outputId": "67c545ee-0f37-4cc7-ca2f-d6a43cc505d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m960.9/960.9 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApWbYUJ47js4",
        "outputId": "3a98c559-3091-469c-bc2b-75892aea3531"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Image Error] Row 7070 image failed: [Errno 5] Input/output error: '/content/drive/MyDrive/Roomify/data/processed/images/img_104241.jpg'[Image Error] Row 4752 image failed: [Errno 5] Input/output error: '/content/drive/MyDrive/Roomify/data/processed/images/img_101923.jpg'\n",
            "\n",
            "[Image Error] Row 16379 image failed: [Errno 5] Input/output error: '/content/drive/MyDrive/Roomify/data/processed/images/img_113550.jpg'[Image Error] Row 3948 image failed: [Errno 5] Input/output error: '/content/drive/MyDrive/Roomify/data/processed/images/img_101119.jpg'\n",
            "\n",
            "[Image Error] Row 9029 image failed: [Errno 5] Input/output error: '/content/drive/MyDrive/Roomify/data/processed/images/img_106200.jpg'[Image Error] Row 8193 image failed: [Errno 5] Input/output error: '/content/drive/MyDrive/Roomify/data/processed/images/img_105364.jpg'\n",
            "\n",
            "✅ Batch loaded!\n",
            "Image shape: torch.Size([16, 3, 256, 256])\n",
            "Prompt sample: ['[Corrupted image]', '[Corrupted image]']\n",
            "Mask shape: torch.Size([16, 1, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Roomify\")\n",
        "from datasets.roomify_datamodule import RoomifyDataModule\n",
        "\n",
        "dm = RoomifyDataModule(\n",
        "    csv_path=\"/content/drive/MyDrive/Roomify/data/processed/unified_prompts.csv\",\n",
        "    root_dir=\"/content/drive/MyDrive/Roomify/data/processed/\",\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "dm.setup()\n",
        "loader = dm.train_dataloader()\n",
        "\n",
        "batch = next(iter(loader))\n",
        "print(\"✅ Batch loaded!\")\n",
        "print(\"Image shape:\", batch[\"image\"].shape)\n",
        "print(\"Prompt sample:\", batch[\"text\"][:2])\n",
        "print(\"Mask shape:\", batch[\"mask\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXbnLfi1nx8w"
      },
      "source": [
        "# **Model Architecture Design**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2Z-I8LEbmps",
        "outputId": "8a8af3e4-d8c9-4740-85a8-ecc3582108dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m133.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.3.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (955.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m955.6/955.6 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 sympy-1.13.3 torch-2.7.0+cu118 torchaudio-2.7.0+cu118 torchvision-0.22.0+cu118 triton-3.3.0\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-kfwu940v\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-kfwu940v\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.7.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.22.0+cu118)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=9b2b79ae139da30e9ca68837aa05afb3b6f6f2a21cd199faa89f4598570f7b52\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o8b0r8vm/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.3.1\n"
          ]
        }
      ],
      "source": [
        "# ✅ إعادة تثبيت PyTorch متوافقة مع A100 (CUDA 11.8)\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# ✅ إعادة تثبيت CLIP من OpenAI (بعد PyTorch)\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "# ✅ إعادة تشغيل الجلسة تلقائيًا (مطلوبة بعد التثبيت)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "J7dRGfA-qoBF",
        "outputId": "94d1dd77-f02c-4d4c-daf5-cd6a6c5cf5b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.10)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mroomify6\u001b[0m (\u001b[33mroomify6-cairo-higher-institute-for-engineering\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5crIo7sEFrPU",
        "outputId": "0d4ba12b-71ac-4e2b-c0c9-2b4de498235a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (2.7.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2025.3.2)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.13.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.18.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch_lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.20.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
            "Successfully installed lightning-utilities-0.14.3 pytorch_lightning-2.5.1.post0 torchmetrics-1.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWQE25M2QwXe",
        "outputId": "62890c5b-069c-48f2-eda1-134eb23098af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.7.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.7.0+cu118)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.14.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhpgohWjc_aN",
        "outputId": "55bd21ac-1c16-4047-891b-318b41522c24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-fidelity\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (11.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (1.15.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (2.7.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (0.22.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch->torch-fidelity) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch->torch-fidelity) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch->torch-fidelity) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torch-fidelity) (3.0.2)\n",
            "Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: torch-fidelity\n",
            "Successfully installed torch-fidelity-0.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-fidelity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3dOVtmXbpph",
        "outputId": "f44ba7ce-4b7e-4e72-e370-8a97114ee6d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.0+cu118)\n",
            "Requirement already satisfied: clip in /usr/local/lib/python3.11/dist-packages (1.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip) (4.67.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision clip pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm wandb ftfy regex matplotlib pillow"
      ],
      "metadata": {
        "id": "zxvLw1srvrYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2285a51f-78c1-4220-94c0-319662f46e4a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.10)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision wandb tqdm pytorch-msssim lpips"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLMwB41F3xHk",
        "outputId": "28debfa7-32cf-4e33-d878-79ff8800d94a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.0+cu118)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting pytorch-msssim\n",
            "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from lpips) (1.15.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-msssim, lpips\n",
            "Successfully installed lpips-0.1.4 pytorch-msssim-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHkxRk9iCzZy",
        "outputId": "7aa0af16-2ce1-47d2-8556-c6faf748e877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.4.26)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ],
      "source": [
        "!pip install deep-translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-DsMSj_Qv21"
      },
      "outputs": [],
      "source": [
        "!python /content/drive/MyDrive/Roomify/inference/inference.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OnLLdjxYMpH",
        "outputId": "89745d48-fc9f-465f-9a94-0464da93e7e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Roomify\")\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets.roomify_dataset import RoomifyDataset\n",
        "from models.generator import ConditionalUNetGenerator\n",
        "from models.discriminator import RoomifyDiscriminator\n",
        "from models.clip_encoder import CLIPTextEncoder\n",
        "from training.trainer import RoomifyGANTrainer\n",
        "\n",
        "# Set up directories with version number to avoid overwriting\n",
        "VERSION = \"v2_extreme\"\n",
        "CHECKPOINT_DIR = f\"/content/drive/MyDrive/Roomify/checkpoints_{VERSION}\"\n",
        "GENERATED_DIR = f\"/content/drive/MyDrive/Roomify/generated_{VERSION}\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(GENERATED_DIR, exist_ok=True)\n",
        "\n",
        "# Set device\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Load dataset with caching\n",
        "dataset = RoomifyDataset(\n",
        "    csv_path=\"/content/drive/MyDrive/Roomify/data/processed/unified_prompts.csv\",\n",
        "    root_dir=\"/content/drive/MyDrive/Roomify/data/processed\",\n",
        "    image_size=(256, 256),\n",
        "    use_cache=True,  # Enable caching\n",
        "    cache_size=200   # Cache size\n",
        ")\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    drop_last=True  # Avoid batch size issues\n",
        ")\n",
        "print(f\"Dataset loaded with {len(dataset)} samples\")\n",
        "\n",
        "# Create models\n",
        "generator = ConditionalUNetGenerator(in_channels=6, out_channels=3).to(DEVICE)\n",
        "discriminator = RoomifyDiscriminator(in_channels=6, text_dim=512).to(DEVICE)\n",
        "clip_encoder = CLIPTextEncoder(device=DEVICE)\n",
        "\n",
        "# Create trainer with extreme settings\n",
        "trainer = RoomifyGANTrainer(\n",
        "    generator=generator,\n",
        "    discriminator=discriminator,\n",
        "    text_encoder=clip_encoder,\n",
        "    dataloader=dataloader,\n",
        "    device=DEVICE,\n",
        "    g_lr=2e-4,\n",
        "    d_lr=3e-4,\n",
        "    ckpt_dir=CHECKPOINT_DIR,\n",
        "    image_save_dir=GENERATED_DIR,\n",
        "    project_name=f\"roomify-{VERSION}\"\n",
        ")\n",
        "\n",
        "# Train for 40 epochs\n",
        "trainer.train(num_epochs=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAUGy3eQ6Mh3",
        "outputId": "dda7aa9d-6300-471c-cae9-75f802b7ad0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📥 Loading images...\n",
            "📸 Fake Images: torch.Size([97, 3, 256, 256])\n",
            "🏞️ Real Images: torch.Size([500, 3, 256, 256])\n",
            "\n",
            "📊 Calculating FID Score...\n",
            "\n",
            "🎯 Final FID Score = 1.8786\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/Roomify/models/compute_FID.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AarstB-HeCu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Roomify\")\n",
        "\n",
        "def test_inference(\n",
        "    image_path,\n",
        "    prompt,\n",
        "    checkpoint_path=None,\n",
        "    output_path=None,\n",
        "    transformation_strength=1.0,\n",
        "    show_attention_maps=False,\n",
        "    show_intermediate=False,\n",
        "    side_by_side=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Run inference on a single image with the specified text prompt.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image\n",
        "        prompt (str): Text description of the desired transformation\n",
        "        checkpoint_path (str, optional): Path to a specific model checkpoint\n",
        "        output_path (str, optional): Path to save the output image\n",
        "        transformation_strength (float): Controls the intensity of the transformation (0.0-1.0)\n",
        "        show_attention_maps (bool): Whether to visualize attention maps\n",
        "        show_intermediate (bool): Whether to show intermediate transformation steps\n",
        "        side_by_side (bool): Whether to display before/after comparison\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the saved output image\n",
        "    \"\"\"\n",
        "    # Validate input image\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Input image not found: {image_path}\")\n",
        "\n",
        "    valid_extensions = ['.jpg', '.jpeg', '.png']\n",
        "    if not any(image_path.lower().endswith(ext) for ext in valid_extensions):\n",
        "        raise ValueError(f\"Unsupported image format. Please use: {valid_extensions}\")\n",
        "\n",
        "    # Set default output path if not provided\n",
        "    if output_path is None:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        output_dir = os.path.join(os.path.dirname(image_path), \"outputs\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        output_path = os.path.join(output_dir, f\"transformed_{timestamp}.jpg\")\n",
        "\n",
        "    # Import necessary modules from your project\n",
        "    try:\n",
        "        from models.generator import Generator\n",
        "        from models.text_encoder import TextEncoder\n",
        "        from utils.image_processing import preprocess_image, postprocess_image\n",
        "    except ImportError as e:\n",
        "        print(f\"Error importing project modules: {e}\")\n",
        "        print(\"Make sure your project path is correctly set in sys.path\")\n",
        "        return None\n",
        "\n",
        "    # Device configuration\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load model\n",
        "    print(f\"Loading model checkpoint...\")\n",
        "    try:\n",
        "        # Initialize text encoder\n",
        "        text_encoder = TextEncoder().to(device)\n",
        "\n",
        "        # Initialize generator\n",
        "        generator = Generator().to(device)\n",
        "\n",
        "        # Load checkpoint\n",
        "        if checkpoint_path is None:\n",
        "            # Try to find the latest checkpoint\n",
        "            checkpoint_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), \"checkpoints\")\n",
        "            checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
        "            if not checkpoints:\n",
        "                raise FileNotFoundError(f\"No checkpoints found in {checkpoint_dir}\")\n",
        "\n",
        "            latest_checkpoint = sorted(checkpoints)[-1]\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
        "\n",
        "        print(f\"Loading checkpoint: {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        generator.load_state_dict(checkpoint['generator'])\n",
        "        text_encoder.load_state_dict(checkpoint['text_encoder'])\n",
        "\n",
        "        # Set to evaluation mode\n",
        "        generator.eval()\n",
        "        text_encoder.eval()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Process the image\n",
        "    print(f\"Processing image with prompt: '{prompt}'\")\n",
        "    try:\n",
        "        # Load and preprocess the image\n",
        "        input_image = Image.open(image_path).convert('RGB')\n",
        "        processed_image = preprocess_image(input_image).to(device)\n",
        "\n",
        "        # Encode the text prompt\n",
        "        text_embedding = text_encoder(prompt)\n",
        "\n",
        "        # Generate the transformed image\n",
        "        with torch.no_grad():\n",
        "            if show_intermediate:\n",
        "                # For visualization of intermediate steps\n",
        "                intermediate_outputs = []\n",
        "                transformed_image, intermediates = generator(\n",
        "                    processed_image,\n",
        "                    text_embedding,\n",
        "                    transformation_strength=transformation_strength,\n",
        "                    return_intermediates=True\n",
        "                )\n",
        "                intermediate_outputs = intermediates\n",
        "            else:\n",
        "                transformed_image = generator(\n",
        "                    processed_image,\n",
        "                    text_embedding,\n",
        "                    transformation_strength=transformation_strength\n",
        "                )\n",
        "\n",
        "            # Get attention maps if requested\n",
        "            attention_maps = None\n",
        "            if show_attention_maps:\n",
        "                attention_maps = generator.get_attention_maps(processed_image, text_embedding)\n",
        "\n",
        "        # Convert to output image\n",
        "        output_image = postprocess_image(transformed_image)\n",
        "\n",
        "        # Save the output\n",
        "        output_image.save(output_path)\n",
        "        print(f\"Transformation complete. Output saved to: {output_path}\")\n",
        "\n",
        "        # Visualization\n",
        "        if side_by_side or show_attention_maps or show_intermediate:\n",
        "            plt.figure(figsize=(15, 10))\n",
        "\n",
        "            if side_by_side:\n",
        "                plt.subplot(1, 2, 1)\n",
        "                plt.title(\"Original Image\")\n",
        "                plt.imshow(np.array(input_image))\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.subplot(1, 2, 2)\n",
        "                plt.title(f\"Transformed: {prompt}\")\n",
        "                plt.imshow(np.array(output_image))\n",
        "                plt.axis('off')\n",
        "\n",
        "            if show_attention_maps and attention_maps is not None:\n",
        "                plt.figure(figsize=(15, 5))\n",
        "                plt.title(\"Attention Maps\")\n",
        "                for i, attn_map in enumerate(attention_maps):\n",
        "                    plt.subplot(1, len(attention_maps), i+1)\n",
        "                    plt.imshow(attn_map.cpu().numpy(), cmap='viridis')\n",
        "                    plt.axis('off')\n",
        "\n",
        "            if show_intermediate and intermediate_outputs:\n",
        "                plt.figure(figsize=(15, 5))\n",
        "                plt.title(\"Transformation Steps\")\n",
        "                for i, img in enumerate(intermediate_outputs):\n",
        "                    plt.subplot(1, len(intermediate_outputs), i+1)\n",
        "                    plt.imshow(postprocess_image(img))\n",
        "                    plt.title(f\"Step {i+1}\")\n",
        "                    plt.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        return output_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during inference: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "\n",
        "def batch_inference(\n",
        "    image_folder,\n",
        "    prompts_list=None,\n",
        "    prompt=None,\n",
        "    output_folder=None,\n",
        "    checkpoint_path=None,\n",
        "    transformation_strength=1.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Process multiple images with corresponding prompts.\n",
        "\n",
        "    Args:\n",
        "        image_folder (str): Path to folder containing input images\n",
        "        prompts_list (list, optional): List of prompts corresponding to each image\n",
        "        prompt (str, optional): Single prompt to apply to all images\n",
        "        output_folder (str, optional): Path to save output images\n",
        "        checkpoint_path (str, optional): Path to model checkpoint\n",
        "        transformation_strength (float): Transformation intensity (0.0-1.0)\n",
        "\n",
        "    Returns:\n",
        "        list: Paths to all generated images\n",
        "    \"\"\"\n",
        "    if not os.path.exists(image_folder):\n",
        "        raise FileNotFoundError(f\"Image folder not found: {image_folder}\")\n",
        "\n",
        "    if prompts_list is None and prompt is None:\n",
        "        raise ValueError(\"Either prompts_list or prompt must be provided\")\n",
        "\n",
        "    # Set default output folder\n",
        "    if output_folder is None:\n",
        "        output_folder = os.path.join(image_folder, \"batch_outputs\")\n",
        "\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Get all images in the folder\n",
        "    valid_extensions = ['.jpg', '.jpeg', '.png']\n",
        "    image_files = [\n",
        "        f for f in os.listdir(image_folder)\n",
        "        if any(f.lower().endswith(ext) for ext in valid_extensions)\n",
        "    ]\n",
        "\n",
        "    if not image_files:\n",
        "        print(f\"No valid images found in {image_folder}\")\n",
        "        return []\n",
        "\n",
        "    # Prepare prompts\n",
        "    if prompts_list is None:\n",
        "        prompts_list = [prompt] * len(image_files)\n",
        "    elif len(prompts_list) < len(image_files):\n",
        "        # Extend prompts list if needed\n",
        "        prompts_list.extend([prompts_list[-1]] * (len(image_files) - len(prompts_list)))\n",
        "\n",
        "    # Process each image\n",
        "    output_paths = []\n",
        "    for i, (image_file, img_prompt) in enumerate(zip(image_files, prompts_list)):\n",
        "        print(f\"\\nProcessing image {i+1}/{len(image_files)}: {image_file}\")\n",
        "        image_path = os.path.join(image_folder, image_file)\n",
        "        output_path = os.path.join(output_folder, f\"transformed_{os.path.splitext(image_file)[0]}.jpg\")\n",
        "\n",
        "        result = test_inference(\n",
        "            image_path=image_path,\n",
        "            prompt=img_prompt,\n",
        "            checkpoint_path=checkpoint_path,\n",
        "            output_path=output_path,\n",
        "            transformation_strength=transformation_strength,\n",
        "            side_by_side=False,\n",
        "            show_attention_maps=False,\n",
        "            show_intermediate=False\n",
        "        )\n",
        "\n",
        "        if result:\n",
        "            output_paths.append(result)\n",
        "\n",
        "    print(f\"\\nBatch processing complete. {len(output_paths)} images generated in {output_folder}\")\n",
        "    return output_paths\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Single image inference\n",
        "    test_inference(\n",
        "        image_path=\"/content/drive/MyDrive/Roomify/inference/sample_input.jpg\",\n",
        "        prompt=\"change the background of this room to something woody\",\n",
        "        transformation_strength=0.8,\n",
        "        side_by_side=True\n",
        "    )\n",
        "\n",
        "    # Batch processing example\n",
        "    # batch_inference(\n",
        "    #     image_folder=\"/content/drive/MyDrive/Roomify/inference/sample_images\",\n",
        "    #     prompt=\"make this room look more modern and minimalist\",\n",
        "    #     transformation_strength=0.7\n",
        "    # )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR4w4aMbCATB",
        "outputId": "caa46d58-d5a7-4def-98df-57922500139a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and boosting the model...\n",
            "Boosting text_proj_inc.0.weight\n",
            "Boosting text_proj_inc.2.weight\n",
            "Boosting text_proj_down1.0.weight\n",
            "Boosting text_proj_down1.2.weight\n",
            "Boosting text_proj_down2.0.weight\n",
            "Boosting text_proj_down2.2.weight\n",
            "Boosting text_proj_down3.0.weight\n",
            "Boosting text_proj_down3.2.weight\n",
            "Boosting text_proj_down4.0.weight\n",
            "Boosting text_proj_down4.2.weight\n",
            "Boosting up1.text_proj.0.weight\n",
            "Boosting up1.text_proj.2.weight\n",
            "Boosting up1.text_proj.4.weight\n",
            "Boosting up1.text_to_feature.0.weight\n",
            "Boosting up1.text_to_feature.2.weight\n",
            "Boosting up2.text_proj.0.weight\n",
            "Boosting up2.text_proj.2.weight\n",
            "Boosting up2.text_proj.4.weight\n",
            "Boosting up2.text_to_feature.0.weight\n",
            "Boosting up2.text_to_feature.2.weight\n",
            "Boosting up3.text_proj.0.weight\n",
            "Boosting up3.text_proj.2.weight\n",
            "Boosting up3.text_proj.4.weight\n",
            "Boosting up3.text_to_feature.0.weight\n",
            "Boosting up3.text_to_feature.2.weight\n",
            "Boosting up4.text_proj.0.weight\n",
            "Boosting up4.text_proj.2.weight\n",
            "Boosting up4.text_proj.4.weight\n",
            "Boosting up4.text_to_feature.0.weight\n",
            "Boosting up4.text_to_feature.2.weight\n",
            "Boosted 30 text-related parameter weights\n",
            "Saved boosted model to /content/drive/MyDrive/Roomify/checkpoints_enhanced/generator_epoch50_boosted.pth\n",
            "\n",
            "Running inference with boosted model...\n",
            "✅ CLIP model loaded successfully\n",
            "Processing prompt: 'transform this room into a cabin with wooden walls'\n",
            "Average pixel difference: 0.358023\n",
            "Saved to: /content/drive/MyDrive/Roomify/boosted_test_results/boosted_0_transform_this_room_into_a_cab.png\n",
            "Processing prompt: 'add rustic wooden planks to the wall'\n",
            "Average pixel difference: 0.358023\n",
            "Saved to: /content/drive/MyDrive/Roomify/boosted_test_results/boosted_1_add_rustic_wooden_planks_to_th.png\n",
            "Processing prompt: 'convert to dark stone walls'\n",
            "Average pixel difference: 0.358023\n",
            "Saved to: /content/drive/MyDrive/Roomify/boosted_test_results/boosted_2_convert_to_dark_stone_walls.png\n",
            "Processing prompt: 'make the walls bright blue'\n",
            "Average pixel difference: 0.358023\n",
            "Saved to: /content/drive/MyDrive/Roomify/boosted_test_results/boosted_3_make_the_walls_bright_blue.png\n",
            "Processing prompt: 'change to industrial style with exposed brick'\n",
            "Average pixel difference: 0.358023\n",
            "Saved to: /content/drive/MyDrive/Roomify/boosted_test_results/boosted_4_change_to_industrial_style_wit.png\n",
            "\n",
            "Saved comparison grid to: /content/drive/MyDrive/Roomify/boosted_test_results/comparison_grid.png\n",
            "\n",
            "✅ Inference complete! Check the results in: /content/drive/MyDrive/Roomify/boosted_test_results\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import clip\n",
        "\n",
        "sys.path.append(\"/content/drive/MyDrive/Roomify\")\n",
        "\n",
        "# Part 1: Load and modify the model to boost text influence\n",
        "class CLIPTextEncoder(torch.nn.Module):\n",
        "    def __init__(self, device=\"cuda\", model_name=\"ViT-B/32\", use_cache=True):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "\n",
        "        # Load CLIP\n",
        "        try:\n",
        "            self.model, _ = clip.load(model_name, device=device)\n",
        "            self.model = self.model.float()\n",
        "            self.model.eval()\n",
        "            self.clip_available = True\n",
        "            print(\"✅ CLIP model loaded successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load CLIP model: {e}\")\n",
        "            self.clip_available = False\n",
        "\n",
        "        self.use_cache = use_cache\n",
        "        self.embedding_cache = {}\n",
        "\n",
        "        # Enhanced text projection\n",
        "        self.text_enhancement = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512, 512),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(512, 1024),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(1024, 512)\n",
        "        )\n",
        "        self.text_enhancement.to(device)\n",
        "\n",
        "    def clear_cache(self):\n",
        "        self.embedding_cache = {}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, texts, temperature=2.5, enhance=True):\n",
        "        \"\"\"Process text with higher temperature by default\"\"\"\n",
        "        if not isinstance(texts, list):\n",
        "            texts = [texts]\n",
        "\n",
        "        if self.use_cache:\n",
        "            cached_results = []\n",
        "            texts_to_process = []\n",
        "            indices = []\n",
        "\n",
        "            for i, text in enumerate(texts):\n",
        "                if text in self.embedding_cache:\n",
        "                    cached_results.append(self.embedding_cache[text])\n",
        "                else:\n",
        "                    texts_to_process.append(text)\n",
        "                    indices.append(i)\n",
        "\n",
        "            if len(texts_to_process) == 0:\n",
        "                embeddings = torch.stack(cached_results)\n",
        "                return embeddings\n",
        "\n",
        "            if self.clip_available:\n",
        "                tokenized = clip.tokenize(texts_to_process).to(self.device)\n",
        "            else:\n",
        "                tokenized = torch.ones((len(texts_to_process), 77), dtype=torch.long, device=self.device)\n",
        "        else:\n",
        "            if self.clip_available:\n",
        "                tokenized = clip.tokenize(texts).to(self.device)\n",
        "            else:\n",
        "                tokenized = torch.ones((len(texts), 77), dtype=torch.long, device=self.device)\n",
        "\n",
        "        if self.clip_available:\n",
        "            embeddings = self.model.encode_text(tokenized)\n",
        "        else:\n",
        "            embeddings = torch.randn(len(tokenized), 512, device=self.device)\n",
        "\n",
        "        if temperature != 1.0:\n",
        "            noise_scale = (temperature - 1.0) * 0.2  # Increased noise scale\n",
        "            embeddings = embeddings + torch.randn_like(embeddings) * noise_scale\n",
        "\n",
        "        if enhance:\n",
        "            embeddings = self.text_enhancement(embeddings)\n",
        "\n",
        "        embeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        if self.use_cache and len(texts_to_process) > 0:\n",
        "            for i, text in enumerate(texts_to_process):\n",
        "                self.embedding_cache[text] = embeddings[i]\n",
        "\n",
        "            if len(cached_results) > 0:\n",
        "                all_embeddings = torch.zeros(len(texts), embeddings.shape[1], device=embeddings.device)\n",
        "\n",
        "                for i, orig_idx in enumerate(indices):\n",
        "                    all_embeddings[orig_idx] = embeddings[i]\n",
        "\n",
        "                cache_idx = 0\n",
        "                for i in range(len(texts)):\n",
        "                    if i not in indices:\n",
        "                        all_embeddings[i] = cached_results[cache_idx]\n",
        "                        cache_idx += 1\n",
        "\n",
        "                embeddings = all_embeddings\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "# Step 1: Set up directories\n",
        "output_dir = \"/content/drive/MyDrive/Roomify/boosted_test_results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Step 2: Boost the model weights\n",
        "from models.generator import ConditionalUNetGenerator\n",
        "checkpoint_path = \"/content/drive/MyDrive/Roomify/checkpoints_enhanced/generator_epoch50.pth\"\n",
        "boosted_checkpoint = \"/content/drive/MyDrive/Roomify/checkpoints_enhanced/generator_epoch50_boosted.pth\"\n",
        "\n",
        "# Load model\n",
        "print(\"Loading and boosting the model...\")\n",
        "model = ConditionalUNetGenerator().to(\"cuda\")\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=\"cuda\"))\n",
        "\n",
        "# Boost text influence dramatically\n",
        "count = 0\n",
        "for name, param in model.named_parameters():\n",
        "    if \"text_to_feature\" in name or \"text_proj\" in name:\n",
        "        if \"weight\" in name:\n",
        "            print(f\"Boosting {name}\")\n",
        "            param.data *= 10.0  # 10x stronger!\n",
        "            count += 1\n",
        "\n",
        "print(f\"Boosted {count} text-related parameter weights\")\n",
        "\n",
        "# Save boosted model\n",
        "torch.save(model.state_dict(), boosted_checkpoint)\n",
        "print(f\"Saved boosted model to {boosted_checkpoint}\")\n",
        "\n",
        "# Step 3: Run inference with boosted model\n",
        "print(\"\\nRunning inference with boosted model...\")\n",
        "\n",
        "# Load image\n",
        "image_path = \"/content/drive/MyDrive/Roomify/inference/sample_input.jpg\"\n",
        "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
        "image = transform(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(\"cuda\")\n",
        "\n",
        "# Create mask (central area)\n",
        "mask = torch.zeros_like(image)\n",
        "h, w = image.shape[2:]\n",
        "y1, y2 = int(h * 0.2), int(h * 0.8)\n",
        "x1, x2 = int(w * 0.2), int(w * 0.8)\n",
        "mask[:, :, y1:y2, x1:x2] = 1.0\n",
        "mask = mask.to(\"cuda\")\n",
        "\n",
        "# Initialize encoders and load boosted model\n",
        "text_encoder = CLIPTextEncoder(device=\"cuda\")\n",
        "model = ConditionalUNetGenerator().to(\"cuda\")\n",
        "model.load_state_dict(torch.load(boosted_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "# List of test prompts\n",
        "test_prompts = [\n",
        "    \"transform this room into a cabin with wooden walls\",\n",
        "    \"add rustic wooden planks to the wall\",\n",
        "    \"convert to dark stone walls\",\n",
        "    \"make the walls bright blue\",\n",
        "    \"change to industrial style with exposed brick\"\n",
        "]\n",
        "\n",
        "# Process each prompt\n",
        "results = []\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"Processing prompt: '{prompt}'\")\n",
        "\n",
        "    # Get text embedding with high temperature\n",
        "    with torch.no_grad():\n",
        "        text_embedding = text_encoder(prompt, temperature=2.5)\n",
        "\n",
        "        # Generate image\n",
        "        output = model(image, mask, text_embedding)\n",
        "        output = torch.clamp(output, 0, 1)\n",
        "\n",
        "    # Save result\n",
        "    prompt_filename = prompt.replace(\" \", \"_\").replace(\"'\", \"\")[:30]\n",
        "    output_path = os.path.join(output_dir, f\"boosted_{i}_{prompt_filename}.png\")\n",
        "    save_image(output, output_path)\n",
        "\n",
        "    # Save mask for reference (first prompt only)\n",
        "    if i == 0:\n",
        "        mask_path = os.path.join(output_dir, \"mask.png\")\n",
        "        save_image(mask, mask_path)\n",
        "        original_path = os.path.join(output_dir, \"original.png\")\n",
        "        save_image(image, original_path)\n",
        "\n",
        "    # Calculate difference\n",
        "    diff = torch.abs(output - image).mean().item()\n",
        "    print(f\"Average pixel difference: {diff:.6f}\")\n",
        "    print(f\"Saved to: {output_path}\")\n",
        "\n",
        "    results.append((prompt, output[0], diff))\n",
        "\n",
        "# Create and save comparison grid\n",
        "all_images = [image[0]] + [r[1] for r in results]\n",
        "grid = torch.stack(list(all_images))\n",
        "grid_path = os.path.join(output_dir, \"comparison_grid.png\")\n",
        "save_image(grid, grid_path, nrow=3)\n",
        "print(f\"\\nSaved comparison grid to: {grid_path}\")\n",
        "\n",
        "print(\"\\n✅ Inference complete! Check the results in:\", output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVsWXUDdn24N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "298e9d96-3679-447e-932a-8fe7582657d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved mask to /content/drive/MyDrive/Roomify/diagnostic_test/mask_0.png\n",
            "✓ Saved modified image to /content/drive/MyDrive/Roomify/diagnostic_test/modified_0.png\n",
            "✓ Saved original to /content/drive/MyDrive/Roomify/diagnostic_test/original.png\n",
            "✓ Saved mask to /content/drive/MyDrive/Roomify/diagnostic_test/mask_1.png\n",
            "✓ Saved modified image to /content/drive/MyDrive/Roomify/diagnostic_test/modified_1.png\n",
            "✓ Saved mask to /content/drive/MyDrive/Roomify/diagnostic_test/mask_2.png\n",
            "✓ Saved modified image to /content/drive/MyDrive/Roomify/diagnostic_test/modified_2.png\n",
            "\n",
            "Diagnostic test complete! Please check the images in: /content/drive/MyDrive/Roomify/diagnostic_test\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Roomify\")\n",
        "import torch\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Create a test directory\n",
        "test_dir = \"/content/drive/MyDrive/Roomify/diagnostic_test\"\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# 1. Load image and create masks\n",
        "image_path = \"/content/drive/MyDrive/Roomify/inference/sample_input.jpg\"\n",
        "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
        "image = transform(Image.open(image_path).convert(\"RGB\")).unsqueeze(0)\n",
        "\n",
        "# 2. Create a different mask for each test\n",
        "masks = [\n",
        "    torch.ones_like(image),  # Full image\n",
        "    torch.zeros_like(image),  # Empty mask\n",
        "]\n",
        "# Add a middle region mask\n",
        "mask3 = torch.zeros_like(image)\n",
        "mask3[:, :, 50:200, 50:200] = 1.0\n",
        "masks.append(mask3)\n",
        "\n",
        "# 3. Create artificially modified outputs to test save functionality\n",
        "modifications = [\n",
        "    lambda img: img * 0.7,                           # Darker\n",
        "    lambda img: img * 0 + torch.tensor([1,0,0]).view(1,3,1,1),  # Pure red\n",
        "    lambda img: img * (mask3 * 0.5 + (1-mask3))      # Darken masked area only\n",
        "]\n",
        "\n",
        "# 4. Save all test images\n",
        "for i, (mask, mod_fn) in enumerate(zip(masks, modifications)):\n",
        "    # Save mask\n",
        "    mask_path = os.path.join(test_dir, f\"mask_{i}.png\")\n",
        "    save_image(mask, mask_path)\n",
        "    print(f\"✓ Saved mask to {mask_path}\")\n",
        "\n",
        "    # Create and save modified image\n",
        "    modified = mod_fn(image)\n",
        "    mod_path = os.path.join(test_dir, f\"modified_{i}.png\")\n",
        "    save_image(modified, mod_path)\n",
        "    print(f\"✓ Saved modified image to {mod_path}\")\n",
        "\n",
        "    # Save original for reference\n",
        "    if i == 0:\n",
        "        orig_path = os.path.join(test_dir, \"original.png\")\n",
        "        save_image(image, orig_path)\n",
        "        print(f\"✓ Saved original to {orig_path}\")\n",
        "\n",
        "print(\"\\nDiagnostic test complete! Please check the images in:\", test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Roomify\")\n",
        "from models.generator import ConditionalUNetGenerator\n",
        "\n",
        "# 1. Load your trained model\n",
        "checkpoint_path = \"/content/drive/MyDrive/Roomify/checkpoints_enhanced/generator_epoch50.pth\"\n",
        "model = ConditionalUNetGenerator().to(\"cuda\")\n",
        "model.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "# 2. Dramatically increase text influence in the model\n",
        "for name, param in model.named_parameters():\n",
        "    if \"text_to_feature\" in name and \"weight\" in name:\n",
        "        print(f\"Boosting weights for {name}\")\n",
        "        # Multiply text feature weights by 10\n",
        "        param.data *= 10.0\n",
        "\n",
        "# 3. Save the modified model\n",
        "modified_path = \"/content/drive/MyDrive/Roomify/checkpoints_enhanced/generator_epoch50_boosted.pth\"\n",
        "torch.save(model.state_dict(), modified_path)\n",
        "print(f\"Saved modified model to {modified_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doXhWz02vq9B",
        "outputId": "13dc588f-1256-4bcc-a829-b4ee63ad7f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boosting weights for up1.text_to_feature.0.weight\n",
            "Boosting weights for up1.text_to_feature.2.weight\n",
            "Boosting weights for up2.text_to_feature.0.weight\n",
            "Boosting weights for up2.text_to_feature.2.weight\n",
            "Boosting weights for up3.text_to_feature.0.weight\n",
            "Boosting weights for up3.text_to_feature.2.weight\n",
            "Boosting weights for up4.text_to_feature.0.weight\n",
            "Boosting weights for up4.text_to_feature.2.weight\n",
            "Saved modified model to /content/drive/MyDrive/Roomify/checkpoints_enhanced/generator_epoch50_boosted.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Roomify\")\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets.roomify_dataset import RoomifyDataset\n",
        "from models.generator import ConditionalUNetGenerator\n",
        "from models.discriminator import RoomifyDiscriminator\n",
        "from models.clip_encoder import CLIPTextEncoder\n",
        "from training.trainer import RoomifyGANTrainer\n",
        "import gc\n",
        "\n",
        "# Free up memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Set up new directories\n",
        "VERSION = \"extreme_v2\"\n",
        "CHECKPOINT_DIR = f\"/content/drive/MyDrive/Roomify/checkpoints_{VERSION}\"\n",
        "GENERATED_DIR = f\"/content/drive/MyDrive/Roomify/generated_{VERSION}\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(GENERATED_DIR, exist_ok=True)\n",
        "\n",
        "# Set device\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Load dataset\n",
        "dataset = RoomifyDataset(\n",
        "    csv_path=\"/content/drive/MyDrive/Roomify/data/processed/unified_prompts.csv\",\n",
        "    root_dir=\"/content/drive/MyDrive/Roomify/data/processed\",\n",
        "    image_size=(256, 256),\n",
        "    use_cache=True,\n",
        "    cache_size=100\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=6,  # Smaller batch size for Colab\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    drop_last=True\n",
        ")\n",
        "print(f\"Dataset loaded with {len(dataset)} samples\")\n",
        "\n",
        "# Create models\n",
        "generator = ConditionalUNetGenerator(in_channels=6, out_channels=3).to(DEVICE)\n",
        "discriminator = RoomifyDiscriminator(in_channels=6, text_dim=512).to(DEVICE)\n",
        "clip_encoder = CLIPTextEncoder(device=DEVICE)\n",
        "\n",
        "# Create trainer\n",
        "trainer = RoomifyGANTrainer(\n",
        "    generator=generator,\n",
        "    discriminator=discriminator,\n",
        "    text_encoder=clip_encoder,\n",
        "    dataloader=dataloader,\n",
        "    device=DEVICE,\n",
        "    g_lr=2e-4,\n",
        "    d_lr=3e-4,\n",
        "    ckpt_dir=CHECKPOINT_DIR,\n",
        "    image_save_dir=GENERATED_DIR,\n",
        "    project_name=f\"roomify-{VERSION}\"\n",
        ")\n",
        "\n",
        "# Train for 10 epochs at a time\n",
        "TOTAL_EPOCHS = 40\n",
        "EPOCHS_PER_RUN = 10\n",
        "\n",
        "for start_epoch in range(0, TOTAL_EPOCHS, EPOCHS_PER_RUN):\n",
        "    end_epoch = min(start_epoch + EPOCHS_PER_RUN, TOTAL_EPOCHS)\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training epochs {start_epoch+1} to {end_epoch}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Clear cache\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Train\n",
        "    trainer.train(num_epochs=EPOCHS_PER_RUN, resume_epoch=start_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HZHs5ygZezeB",
        "outputId": "b49ab05e-00f3-47cf-e8ac-d6ea903060a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "[Dataset] Dropped 0 empty rows, 0 corrupt/missing files.\n",
            "[Dataset] Loaded 24474 valid samples.\n",
            "Dataset loaded with 24474 samples\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mroomify6\u001b[0m (\u001b[33mroomify6-cairo-higher-institute-for-engineering\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250429_102434-jnvxa6yz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/roomify6-cairo-higher-institute-for-engineering/roomify-extreme_v2/runs/jnvxa6yz' target=\"_blank\">Extreme-Training</a></strong> to <a href='https://wandb.ai/roomify6-cairo-higher-institute-for-engineering/roomify-extreme_v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/roomify6-cairo-higher-institute-for-engineering/roomify-extreme_v2' target=\"_blank\">https://wandb.ai/roomify6-cairo-higher-institute-for-engineering/roomify-extreme_v2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/roomify6-cairo-higher-institute-for-engineering/roomify-extreme_v2/runs/jnvxa6yz' target=\"_blank\">https://wandb.ai/roomify6-cairo-higher-institute-for-engineering/roomify-extreme_v2/runs/jnvxa6yz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Training epochs 1 to 10\n",
            "==================================================\n",
            "\n",
            "🚀 Starting training...\n",
            "\n",
            "=== Epoch 1/10 ===\n",
            "Current loss weights: L1=0.01, Adv=5.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1: 100%|██████████| 4079/4079 [20:42<00:00,  3.28it/s, g_loss=16.8350, d_loss=0.6935, adv_loss=0.6734, l1_loss=0.1726, perc_loss=0.0154, l1_weight=0.0100, adv_weight=5.0000]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Epoch 1 Summary:\n",
            "Generator Loss: 19.4164\n",
            "Discriminator Loss: 0.9299\n",
            "Adversarial Loss: 0.7766\n",
            "L1 Loss: 0.1213\n",
            "L1 Weight: 0.0100\n",
            "💾 Checkpoints saved for epoch 1\n",
            "\n",
            "=== Epoch 2/10 ===\n",
            "Current loss weights: L1=0.01, Adv=4.85\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2: 100%|██████████| 4079/4079 [20:00<00:00,  3.40it/s, g_loss=16.0649, d_loss=0.6944, adv_loss=0.6625, l1_loss=0.0589, perc_loss=0.0053, l1_weight=0.0145, adv_weight=4.8500]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Epoch 2 Summary:\n",
            "Generator Loss: 16.8166\n",
            "Discriminator Loss: 0.6934\n",
            "Adversarial Loss: 0.6935\n",
            "L1 Loss: 0.1018\n",
            "L1 Weight: 0.0145\n",
            "💾 Checkpoints saved for epoch 2\n",
            "\n",
            "=== Epoch 3/10 ===\n",
            "Current loss weights: L1=0.02, Adv=4.70\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3: 100%|██████████| 4079/4079 [20:04<00:00,  3.39it/s, g_loss=16.2316, d_loss=0.6932, adv_loss=0.6907, l1_loss=0.0634, perc_loss=0.0054, l1_weight=0.0190, adv_weight=4.7000]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Epoch 3 Summary:\n",
            "Generator Loss: 18.6496\n",
            "Discriminator Loss: 0.8075\n",
            "Adversarial Loss: 0.7936\n",
            "L1 Loss: 0.0954\n",
            "L1 Weight: 0.0190\n",
            "💾 Checkpoints saved for epoch 3\n",
            "\n",
            "=== Epoch 4/10 ===\n",
            "Current loss weights: L1=0.02, Adv=4.55\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4: 100%|██████████| 4079/4079 [22:07<00:00,  3.07it/s, g_loss=16.2180, d_loss=0.6944, adv_loss=0.7129, l1_loss=0.0432, perc_loss=0.0035, l1_weight=0.0235, adv_weight=4.5500]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Epoch 4 Summary:\n",
            "Generator Loss: 20.3350\n",
            "Discriminator Loss: 0.9502\n",
            "Adversarial Loss: 0.8938\n",
            "L1 Loss: 0.0479\n",
            "L1 Weight: 0.0235\n",
            "💾 Checkpoints saved for epoch 4\n",
            "\n",
            "=== Epoch 5/10 ===\n",
            "Current loss weights: L1=0.03, Adv=4.40\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5: 100%|██████████| 4079/4079 [24:49<00:00,  2.74it/s, g_loss=15.5786, d_loss=0.6933, adv_loss=0.7081, l1_loss=0.0220, perc_loss=0.0019, l1_weight=0.0280, adv_weight=4.4000]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Epoch 5 Summary:\n",
            "Generator Loss: 15.3269\n",
            "Discriminator Loss: 0.6967\n",
            "Adversarial Loss: 0.6967\n",
            "L1 Loss: 0.0288\n",
            "L1 Weight: 0.0280\n",
            "💾 Checkpoints saved for epoch 5\n",
            "\n",
            "=== Epoch 6/10 ===\n",
            "Current loss weights: L1=0.03, Adv=4.25\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6: 100%|██████████| 4079/4079 [25:07<00:00,  2.71it/s, g_loss=14.7792, d_loss=0.6932, adv_loss=0.6955, l1_loss=0.0118, perc_loss=0.0011, l1_weight=0.0325, adv_weight=4.2500]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Epoch 6 Summary:\n",
            "Generator Loss: 14.7316\n",
            "Discriminator Loss: 0.6933\n",
            "Adversarial Loss: 0.6932\n",
            "L1 Loss: 0.0209\n",
            "L1 Weight: 0.0325\n",
            "💾 Checkpoints saved for epoch 6\n",
            "\n",
            "=== Epoch 7/10 ===\n",
            "Current loss weights: L1=0.04, Adv=4.10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7: 100%|██████████| 4079/4079 [25:10<00:00,  2.70it/s, g_loss=13.0531, d_loss=0.7002, adv_loss=0.6367, l1_loss=0.0277, perc_loss=0.0021, l1_weight=0.0370, adv_weight=4.1000]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Epoch 7 Summary:\n",
            "Generator Loss: 20.2158\n",
            "Discriminator Loss: 1.2155\n",
            "Adversarial Loss: 0.9861\n",
            "L1 Loss: 0.0176\n",
            "L1 Weight: 0.0370\n",
            "💾 Checkpoints saved for epoch 7\n",
            "\n",
            "=== Epoch 8/10 ===\n",
            "Current loss weights: L1=0.04, Adv=3.95\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8: 100%|██████████| 4079/4079 [25:02<00:00,  2.71it/s, g_loss=13.5114, d_loss=0.6938, adv_loss=0.6841, l1_loss=0.0121, perc_loss=0.0010, l1_weight=0.0415, adv_weight=3.9500]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Epoch 8 Summary:\n",
            "Generator Loss: 17.2888\n",
            "Discriminator Loss: 0.8703\n",
            "Adversarial Loss: 0.8754\n",
            "L1 Loss: 0.0148\n",
            "L1 Weight: 0.0415\n",
            "💾 Checkpoints saved for epoch 8\n",
            "\n",
            "=== Epoch 9/10 ===\n",
            "Current loss weights: L1=0.05, Adv=3.80\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 9: 100%|██████████| 4079/4079 [25:38<00:00,  2.65it/s, g_loss=13.7038, d_loss=0.6937, adv_loss=0.7212, l1_loss=0.0131, perc_loss=0.0011, l1_weight=0.0460, adv_weight=3.8000]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Epoch 9 Summary:\n",
            "Generator Loss: 13.2006\n",
            "Discriminator Loss: 0.6945\n",
            "Adversarial Loss: 0.6948\n",
            "L1 Loss: 0.0133\n",
            "L1 Weight: 0.0460\n",
            "💾 Checkpoints saved for epoch 9\n",
            "\n",
            "=== Epoch 10/10 ===\n",
            "Current loss weights: L1=0.05, Adv=3.65\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 10: 100%|██████████| 4079/4079 [25:24<00:00,  2.67it/s, g_loss=12.7174, d_loss=0.6932, adv_loss=0.6968, l1_loss=0.0182, perc_loss=0.0014, l1_weight=0.0505, adv_weight=3.6500]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Epoch 10 Summary:\n",
            "Generator Loss: 13.6448\n",
            "Discriminator Loss: 0.7453\n",
            "Adversarial Loss: 0.7477\n",
            "L1 Loss: 0.0120\n",
            "L1 Weight: 0.0505\n",
            "💾 Checkpoints saved for epoch 10\n",
            "\n",
            "✅ Training completed!\n",
            "\n",
            "==================================================\n",
            "Training epochs 11 to 20\n",
            "==================================================\n",
            "\n",
            "🚀 Starting training...\n",
            "\n",
            "=== Epoch 11/20 ===\n",
            "Current loss weights: L1=0.06, Adv=3.50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 11: 100%|██████████| 4079/4079 [24:54<00:00,  2.73it/s, g_loss=12.0708, d_loss=0.6932, adv_loss=0.6898, l1_loss=0.0057, perc_loss=0.0005, l1_weight=0.0550, adv_weight=3.5000]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Epoch 11 Summary:\n",
            "Generator Loss: 12.1326\n",
            "Discriminator Loss: 0.6933\n",
            "Adversarial Loss: 0.6933\n",
            "L1 Loss: 0.0087\n",
            "L1 Weight: 0.0550\n",
            "💾 Checkpoints saved for epoch 11\n",
            "\n",
            "=== Epoch 12/20 ===\n",
            "Current loss weights: L1=0.06, Adv=3.35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 12: 100%|██████████| 4079/4079 [24:47<00:00,  2.74it/s, g_loss=11.6083, d_loss=0.6932, adv_loss=0.6930, l1_loss=0.0078, perc_loss=0.0007, l1_weight=0.0595, adv_weight=3.3500]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Epoch 12 Summary:\n",
            "Generator Loss: 11.6716\n",
            "Discriminator Loss: 0.6970\n",
            "Adversarial Loss: 0.6968\n",
            "L1 Loss: 0.0081\n",
            "L1 Weight: 0.0595\n",
            "💾 Checkpoints saved for epoch 12\n",
            "\n",
            "=== Epoch 13/20 ===\n",
            "Current loss weights: L1=0.06, Adv=3.20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 13:  83%|████████▎ | 3404/4079 [20:27<04:12,  2.67it/s, g_loss=11.1696, d_loss=0.6932, adv_loss=0.6981, l1_loss=0.0079, perc_loss=0.0007, l1_weight=0.0640, adv_weight=3.2000]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/Roomify/test_images"
      ],
      "metadata": {
        "id": "d_IanjMnJlTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Roomify\")\n",
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from models.generator import ConditionalUNetGenerator\n",
        "from models.clip_encoder import CLIPTextEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Setup directories\n",
        "test_dir = \"/content/drive/MyDrive/Roomify/test_results\"\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Load your best model\n",
        "model_path = \"/content/drive/MyDrive/Roomify/checkpoints_extreme_v2/generator_epoch8.pth\"  # Use your latest epoch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize models\n",
        "generator = ConditionalUNetGenerator(in_channels=6, out_channels=3).to(device)\n",
        "generator.load_state_dict(torch.load(model_path, map_location=device))\n",
        "generator.eval()\n",
        "\n",
        "text_encoder = CLIPTextEncoder(device=device)\n",
        "\n",
        "# Image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Function to generate images based on prompts\n",
        "def generate_room_variants(image_path, prompts, output_dir, use_mask=True):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load and preprocess image\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # Create mask (default: full image)\n",
        "    if use_mask:\n",
        "        mask = torch.ones_like(img_tensor)\n",
        "    else:\n",
        "        # Create mask for just walls (example)\n",
        "        mask = torch.zeros_like(img_tensor)\n",
        "        # Middle rectangular area (typical walls area)\n",
        "        mask[:, :, 30:226, 30:226] = 1.0\n",
        "\n",
        "    # Save original image\n",
        "    original_path = os.path.join(output_dir, \"original.png\")\n",
        "    save_image(img_tensor, original_path)\n",
        "\n",
        "    # Save mask visualization\n",
        "    mask_path = os.path.join(output_dir, \"mask.png\")\n",
        "    save_image(mask, mask_path)\n",
        "\n",
        "    results = []\n",
        "    all_images = [img_tensor[0]]\n",
        "\n",
        "    # Process each prompt\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        print(f\"Processing prompt: '{prompt}'\")\n",
        "\n",
        "        # Get text embedding with high temperature for diversity\n",
        "        text_embedding = text_encoder(prompt, temperature=2.0)\n",
        "\n",
        "        # Generate image\n",
        "        with torch.no_grad():\n",
        "            output = generator(img_tensor, mask, text_embedding)\n",
        "            output = torch.clamp(output, 0, 1)\n",
        "\n",
        "        # Save output\n",
        "        output_path = os.path.join(output_dir, f\"{i+1}_{prompt.replace(' ', '_')[:30]}.png\")\n",
        "        save_image(output, output_path)\n",
        "\n",
        "        # Calculate difference\n",
        "        diff = torch.abs(output - img_tensor).mean().item()\n",
        "        print(f\"Average difference: {diff:.4f}\")\n",
        "\n",
        "        results.append((prompt, output[0], diff))\n",
        "        all_images.append(output[0])\n",
        "\n",
        "    # Create comparison grid\n",
        "    grid = make_grid(all_images, nrow=3)\n",
        "    grid_path = os.path.join(output_dir, \"comparison_grid.png\")\n",
        "    save_image(grid, grid_path)\n",
        "\n",
        "    # Create difference visualization\n",
        "    diff_images = []\n",
        "    diff_images.append(img_tensor[0])  # Original\n",
        "    for _, img, _ in results:\n",
        "        # Amplify differences for visibility\n",
        "        diff_img = torch.abs(img - img_tensor[0]) * 5\n",
        "        diff_images.append(diff_img)\n",
        "\n",
        "    diff_grid = make_grid(diff_images, nrow=3)\n",
        "    diff_grid_path = os.path.join(output_dir, \"difference_grid.png\")\n",
        "    save_image(diff_grid, diff_grid_path)\n",
        "\n",
        "    print(f\"Results saved to {output_dir}\")\n",
        "    return results\n",
        "\n",
        "# Test with different prompt categories\n",
        "test_images = [\n",
        "    \"/content/drive/MyDrive/Roomify/test_images/023c66ab118a2c487f82c3ac145c69c9.jpg\",\n",
        "    \"/content/drive/MyDrive/Roomify/test_images/2bb101db004211f248fb7f1c0e254fee.jpg\",\n",
        "    \"/content/drive/MyDrive/Roomify/test_images/e216c5d3ba2356676429bcf10bc5245a.jpg\",\n",
        "    \"/content/drive/MyDrive/Roomify/test_images/e216c5d3ba2356676429bcf10bc5245a.jpg\"\n",
        "    # Add more test images\n",
        "]\n",
        "\n",
        "# Define test prompt sets\n",
        "color_prompts = [\n",
        "    \"change the wall color to light blue\",\n",
        "    \"paint the walls dark green\",\n",
        "    \"make the walls bright yellow\",\n",
        "    \"change to white walls with black accents\"\n",
        "]\n",
        "\n",
        "material_prompts = [\n",
        "    \"add wooden panels to the walls\",\n",
        "    \"convert to exposed brick walls\",\n",
        "    \"change to marble wall texture\",\n",
        "    \"add stone texture to the walls\"\n",
        "]\n",
        "\n",
        "style_prompts = [\n",
        "    \"transform to modern minimalist style\",\n",
        "    \"convert to rustic farmhouse style\",\n",
        "    \"change to luxury penthouse style\",\n",
        "    \"redesign as industrial style room\"\n",
        "]\n",
        "\n",
        "# Run tests on each image with different prompt types\n",
        "for img_path in test_images:\n",
        "    img_name = os.path.basename(img_path).split('.')[0]\n",
        "    base_dir = os.path.join(test_dir, img_name)\n",
        "\n",
        "    # Test with color prompts\n",
        "    generate_room_variants(\n",
        "        image_path=img_path,\n",
        "        prompts=color_prompts,\n",
        "        output_dir=os.path.join(base_dir, \"colors\"),\n",
        "        use_mask=False  # Target walls\n",
        "    )\n",
        "\n",
        "    # Test with material prompts\n",
        "    generate_room_variants(\n",
        "        image_path=img_path,\n",
        "        prompts=material_prompts,\n",
        "        output_dir=os.path.join(base_dir, \"materials\"),\n",
        "        use_mask=False\n",
        "    )\n",
        "\n",
        "    # Test with style prompts\n",
        "    generate_room_variants(\n",
        "        image_path=img_path,\n",
        "        prompts=style_prompts,\n",
        "        output_dir=os.path.join(base_dir, \"styles\"),\n",
        "        use_mask=True  # Full image for style changes\n",
        "    )\n",
        "\n",
        "print(\"Testing complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBxDp-ITJmqv",
        "outputId": "7bea1f61-8f27-40be-f6db-486c747d03e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:15<00:00, 23.1MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt: 'change the wall color to light blue'\n",
            "Average difference: 0.1753\n",
            "Processing prompt: 'paint the walls dark green'\n",
            "Average difference: 0.1816\n",
            "Processing prompt: 'make the walls bright yellow'\n",
            "Average difference: 0.1713\n",
            "Processing prompt: 'change to white walls with black accents'\n",
            "Average difference: 0.1755\n",
            "Results saved to /content/drive/MyDrive/Roomify/test_results/023c66ab118a2c487f82c3ac145c69c9/colors\n",
            "Processing prompt: 'add wooden panels to the walls'\n",
            "Average difference: 0.1799\n",
            "Processing prompt: 'convert to exposed brick walls'\n",
            "Average difference: 0.1731\n",
            "Processing prompt: 'change to marble wall texture'\n",
            "Average difference: 0.1720\n",
            "Processing prompt: 'add stone texture to the walls'\n",
            "Average difference: 0.1739\n",
            "Results saved to /content/drive/MyDrive/Roomify/test_results/023c66ab118a2c487f82c3ac145c69c9/materials\n",
            "Processing prompt: 'transform to modern minimalist style'\n",
            "Average difference: 0.1147\n",
            "Processing prompt: 'convert to rustic farmhouse style'\n",
            "Average difference: 0.1246\n",
            "Processing prompt: 'change to luxury penthouse style'\n",
            "Average difference: 0.1276\n",
            "Processing prompt: 'redesign as industrial style room'\n",
            "Average difference: 0.1287\n",
            "Results saved to /content/drive/MyDrive/Roomify/test_results/023c66ab118a2c487f82c3ac145c69c9/styles\n",
            "Processing prompt: 'change the wall color to light blue'\n",
            "Average difference: 0.1955\n",
            "Processing prompt: 'paint the walls dark green'\n",
            "Average difference: 0.1998\n",
            "Processing prompt: 'make the walls bright yellow'\n",
            "Average difference: 0.1919\n",
            "Processing prompt: 'change to white walls with black accents'\n",
            "Average difference: 0.1952\n",
            "Results saved to /content/drive/MyDrive/Roomify/test_results/2bb101db004211f248fb7f1c0e254fee/colors\n",
            "Processing prompt: 'add wooden panels to the walls'\n",
            "Average difference: 0.1991\n",
            "Processing prompt: 'convert to exposed brick walls'\n",
            "Average difference: 0.1930\n",
            "Processing prompt: 'change to marble wall texture'\n",
            "Average difference: 0.1924\n",
            "Processing prompt: 'add stone texture to the walls'\n",
            "Average difference: 0.1934\n",
            "Results saved to /content/drive/MyDrive/Roomify/test_results/2bb101db004211f248fb7f1c0e254fee/materials\n",
            "Processing prompt: 'transform to modern minimalist style'\n",
            "Average difference: 0.1056\n",
            "Processing prompt: 'convert to rustic farmhouse style'\n",
            "Average difference: 0.1141\n",
            "Processing prompt: 'change to luxury penthouse style'\n",
            "Average difference: 0.1168\n",
            "Processing prompt: 'redesign as industrial style room'\n",
            "Average difference: 0.1177\n",
            "Results saved to /content/drive/MyDrive/Roomify/test_results/2bb101db004211f248fb7f1c0e254fee/styles\n",
            "Processing prompt: 'change the wall color to light blue'\n",
            "Average difference: 0.1730\n",
            "Processing prompt: 'paint the walls dark green'\n",
            "Average difference: 0.1788\n",
            "Processing prompt: 'make the walls bright yellow'\n",
            "Average difference: 0.1689\n",
            "Processing prompt: 'change to white walls with black accents'\n",
            "Average difference: 0.1731\n",
            "Results saved to /content/drive/MyDrive/Roomify/test_results/e216c5d3ba2356676429bcf10bc5245a/colors\n",
            "Processing prompt: 'add wooden panels to the walls'\n",
            "Average difference: 0.1774\n",
            "Processing prompt: 'convert to exposed brick walls'\n",
            "Average difference: 0.1706\n",
            "Processing prompt: 'change to marble wall texture'\n",
            "Average difference: 0.1695\n",
            "Processing prompt: 'add stone texture to the walls'\n",
            "Average difference: 0.1713\n",
            "Results saved to /content/drive/MyDrive/Roomify/test_results/e216c5d3ba2356676429bcf10bc5245a/materials\n",
            "Processing prompt: 'transform to modern minimalist style'\n",
            "Average difference: 0.1213\n",
            "Processing prompt: 'convert to rustic farmhouse style'\n",
            "Average difference: 0.1320\n",
            "Processing prompt: 'change to luxury penthouse style'\n",
            "Average difference: 0.1352\n",
            "Processing prompt: 'redesign as industrial style room'\n",
            "Average difference: 0.1365\n",
            "Results saved to /content/drive/MyDrive/Roomify/test_results/e216c5d3ba2356676429bcf10bc5245a/styles\n",
            "Processing prompt: 'change the wall color to light blue'\n",
            "Average difference: 0.1730\n",
            "Processing prompt: 'paint the walls dark green'\n",
            "Average difference: 0.1788\n",
            "Processing prompt: 'make the walls bright yellow'\n",
            "Average difference: 0.1689\n",
            "Processing prompt: 'change to white walls with black accents'\n",
            "Average difference: 0.1731\n",
            "Results saved to /content/drive/MyDrive/Roomify/test_results/e216c5d3ba2356676429bcf10bc5245a/colors\n",
            "Processing prompt: 'add wooden panels to the walls'\n",
            "Average difference: 0.1774\n",
            "Processing prompt: 'convert to exposed brick walls'\n",
            "Average difference: 0.1706\n",
            "Processing prompt: 'change to marble wall texture'\n",
            "Average difference: 0.1695\n",
            "Processing prompt: 'add stone texture to the walls'\n",
            "Average difference: 0.1713\n",
            "Results saved to /content/drive/MyDrive/Roomify/test_results/e216c5d3ba2356676429bcf10bc5245a/materials\n",
            "Processing prompt: 'transform to modern minimalist style'\n",
            "Average difference: 0.1213\n",
            "Processing prompt: 'convert to rustic farmhouse style'\n",
            "Average difference: 0.1320\n",
            "Processing prompt: 'change to luxury penthouse style'\n",
            "Average difference: 0.1352\n",
            "Processing prompt: 'redesign as industrial style room'\n",
            "Average difference: 0.1365\n",
            "Results saved to /content/drive/MyDrive/Roomify/test_results/e216c5d3ba2356676429bcf10bc5245a/styles\n",
            "Testing complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/Roomify/train_stylegan.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yZe4z6vxiXj",
        "outputId": "79315546-3681-4548-d448-5f2265f0b4f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mroomify6\u001b[0m (\u001b[33mroomify6-cairo-higher-institute-for-engineering\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250505_052315-d4xryxzp\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mold-droid-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/roomify6-cairo-higher-institute-for-engineering/roomify-stylegan\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/roomify6-cairo-higher-institute-for-engineering/roomify-stylegan/runs/d4xryxzp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/Roomify/checkpoints_stylegan"
      ],
      "metadata": {
        "id": "JTkuWbUlyMkg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-3cN3-bQyvyF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "euxXDzw-30dJ",
        "M4fq3Y8L3_RE",
        "ZZuSr_wz7lrq",
        "hdFnMABK9uuA",
        "g5H0Qs2B-6zz",
        "3tas8m9fDhr4",
        "MYrJ0w48IpjX",
        "Ujaq9e9sJ4B_",
        "G7lqJbFUiez_"
      ],
      "gpuType": "A100",
      "provenance": [],
      "mount_file_id": "1kMB8zxMuKUGrH5amnt_GHWfjdD9jIa-U",
      "authorship_tag": "ABX9TyM2uY4s7tfKsKMMK9e1owtH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}